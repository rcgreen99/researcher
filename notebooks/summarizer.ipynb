{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset cnn_dailymail (/home/reese/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602513774bee4a5fbd83d281daf9064d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article (excerpt of 500 characters, total length: 4051): \n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n",
      "\n",
      "Summary (length: 281):\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}): \"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):') \n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_summaries_baseline(dataset, metric, column_text=\"article\",\n",
    "#     column_summary=\"highlights\"):\n",
    "#     summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "#     metric.add_batch(predictions=summaries, references=dataset[column_summary])\n",
    "#     score = metric.compute() \n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/reese/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-888acb9a2eb72e89.arrow\n"
     ]
    }
   ],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "# score = evaluate_summaries_baseline(test_sampled, rouge_metric) \n",
    "# rouge_dict = dict((rn, score[rn].mid.fmeasure) \n",
    "#                   for rn in rouge_names) \n",
    "# pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i:i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device,\n",
    "                                   column_text=\"article\",\n",
    "                                   column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size)) \n",
    "    \n",
    "    for article_batch, target_batch in tqdm(\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "            \n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                             attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                             length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            \n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                    clean_up_tokenization_spaces=True)\n",
    "                                    for s in summaries]\n",
    "            \n",
    "            decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries] \n",
    "            \n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute() \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81930af99064503816d669e7bf192d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4787c1e6a81e47f7b51d9fad5e5acdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef696d4926c4556be0205ba0fbb2621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6eaa88d70a4c65953ab9be04dda36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749643fa51f14ef3926a21f34b29b434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]2023-04-02 14:34:20.029323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-02 14:34:21.155062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "2023-04-02 14:34:21.155125: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "2023-04-02 14:34:21.155129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 125/125 [07:34<00:00,  3.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.434779</td>\n",
       "      <td>0.216666</td>\n",
       "      <td>0.31278</td>\n",
       "      <td>0.374219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1    rouge2   rougeL  rougeLsum\n",
       "pegasus  0.434779  0.216666  0.31278   0.374219"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric,\n",
    "                                   model, tokenizer, batch_size=8) \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,   184,   799,  1974,   124,   109,  1778,   113,  7093,  5551,\n",
      "          5906,  6520,  3884,   113,   954,  5394,  8810,   110,   107,   106,\n",
      "           284,   207,  1581,  2492,   124,  1934,   972,   113,   109,  1778,\n",
      "           112,  1595,  4095,   115,  1132,  2230,   124,   109,  7792,  1778,\n",
      "           110,   107,   106,   284,  1433,   449,   121,  1313,   121,   544,\n",
      "           121,  3904,   602,   124,   109,   909,  2459, 53231, 20886,   118,\n",
      "           410,   121, 11412,  5906,  6520,  3884,   110,   107,     1]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We present progress on the task of abstractive summarization of entire fiction novels. We use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom example\n",
    "text = \"\"\"\n",
    "    A major challenge for scaling machine learning is training models to perform\n",
    "    tasks that are very difficult or time-consuming for humans to evaluate. We present\n",
    "    progress on this problem on the task of abstractive summarization of entire fiction\n",
    "    novels. Our method combines learning from human feedback with recursive\n",
    "    task decomposition: we use models trained on smaller parts of the task to assist\n",
    "    humans in giving feedback on the broader task. We collect a large volume of\n",
    "    demonstrations and comparisons from human labelers, and fine-tune GPT-3 using\n",
    "    behavioral cloning and reward modeling to do summarization recursively. At\n",
    "    inference time, the model first summarizes small sections of the book and then\n",
    "    recursively summarizes these summaries to produce a summary of the entire book.\n",
    "    Our human labelers are able to supervise and evaluate the models quickly, despite\n",
    "    not having read the entire books themselves. Our resulting model generates sensible\n",
    "    summaries of entire books, even matching the quality of human-written summaries\n",
    "    in a few cases (∼ 5% of books). We achieve state-of-the-art results on the recent\n",
    "    BookSum dataset for book-length summarization. A zero-shot question-answering\n",
    "    model using these summaries achieves competitive results on the challenging\n",
    "    NarrativeQA benchmark for answering questions about books and movie scripts.\n",
    "    We release datasets of samples from our model.2\n",
    "\"\"\"\n",
    "\n",
    "tokenized_input = tokenizer(text, max_length=1024,  truncation=True,\n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "tokenized_input = tokenizer(text, max_length=1024,  truncation=True,\n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "summary = model.generate(input_ids=tokenized_input[\"input_ids\"].to(device),\n",
    "                    attention_mask=tokenized_input[\"attention_mask\"].to(device),\n",
    "                    length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "decoded_summary = tokenizer.decode(summary[0], skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=True)\n",
    "\n",
    "decoded_summary = decoded_summary.replace(\"<n>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We present progress on the task of abstractive summarization of entire fiction novels. We use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization.\n"
     ]
    }
   ],
   "source": [
    "print(decoded_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
